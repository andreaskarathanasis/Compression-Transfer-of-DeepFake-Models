{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning function\n",
    "def prune_model(model=None, pruning_rate=0):\n",
    "\n",
    "    for _, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):            \n",
    "            prune.l1_unstructured(module, name='weight', amount=pruning_rate, importance_scores=None)\n",
    "\n",
    "# Use function to prune the model\n",
    "prune_model(model, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that removes pruning mask\n",
    "def delete_mask(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
    "            prune.remove(module, 'weight')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Use function to delete model's mask\n",
    "model = delete_mask(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code calculates the distillation loss as part of the distillation process.\n",
    "The full distillation pipeline will be provided in in a future update.\n",
    "'''\n",
    "# Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
    "with torch.no_grad():\n",
    "    teacher_logits = teacher_model(X_train) # X_train is the batched training images\n",
    "                \n",
    "# Forward pass with the student model\n",
    "student_logits = student_model(X_train) # X_train are the batched training images\n",
    "\n",
    "# Soften the student logits by applying softmax first and log() second\n",
    "# T is the \"Temperature\" hyperparameter\n",
    "soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
    "soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
    "\n",
    "# Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
    "soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)\n",
    "\n",
    "# Calculate the true label loss\n",
    "label_loss = loss_fn(student_logits, y_train) # y_train are the batched labels\n",
    "\n",
    "# Weighted sum of the two losses\n",
    "# 'soft_target_loss_weight' and 'ce_loss_weight' are weight hyperparameters for the soft and hard target labels resectively\n",
    "loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
    "\n",
    "# Accumulates the total loss for the epoch for monitoring\n",
    "epoch_loss += loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize the original 'model' in all linear layer to int8 from float32\n",
    "model_int8 = torch.ao.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low Rank Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose a dense layer using SVD\n",
    "def decompose_linear_layer(layer, rank):\n",
    "    weights = layer.weight.data\n",
    "    # SVD decomposition\n",
    "    U, S, V = torch.svd_lowrank(weights)\n",
    "    U_hat, S_hat, V_hat = U[:, :rank], torch.diag(S[:rank]), V[:,:rank]\n",
    "    W1 = torch.mm(S_hat, V_hat.t())\n",
    "\n",
    "    return W1, U_hat, layer.bias.data\n",
    "\n",
    "# Decompose the dense layer\n",
    "# 'original_model' is the model we need to perform the LRF\n",
    "W1, W2, biases = decompose_linear_layer(original_model.fc[1], rank)\n",
    "\n",
    "# Set weights for the decomposed layers\n",
    "# 'model_lrf' represents an instance of the decomposed model class\n",
    "model_lrf.fc[1].weight.data = W1\n",
    "model_lrf.fc1[1].weight.data = W2\n",
    "model_lrf.fc1[1].bias.data = biases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
